# Introduction

Scalable procedurally generated Markov Decision Processes (MDPs) by randomizing the reward matrix and transition matrix. 


# Install

```bash
pip install xenoverse[anymdp]
```

#### For local installation, execute following commands:

```bash
git clone https://github.com/FutureAGI/xenoverse
cd xenoverse
pip install .[anymdp]
```

# Quick Start

## Import

Import and create the AnyMDP environment with 
```python
import gym
import xenoverse.anymdp

env = gym.make("anymdp-v0", max_steps=5000)
```

## Sampling an AnyMDP task
```python
from xenoverse.anymdp import AnyMDPTaskSampler

task = AnyMDPTaskSampler(
        state_space:int=128,      # size of maximum states
        action_space:int=5,       # number of actions
        min_state_space:int=None, # size of minimum states, default is the same as state_space
        reward_noise_choice:list=['normal'], # choice of reward noise type, can select from ['normal', 'binoimial']
        )
env.set_task(task)
env.reset()
```

You might resample a MDP task by keeping the transitions unchanged but sample a new reward matrix by

```python
from xenoverse.anymdp import Resampler
new_task = Resampler(task)
```

## Running the built-in MDP solver
```python
from xenoverse.anymdp import AnyMDPSolverOpt

solver = AnyMDPSolverOpt(env)  # AnyMDPSolverOpt solves the MDP with ground truth rewards and transition matrix
state, info = env.reset()
done = False
while not done:
    action = solver.policy(state)
    state, reward, done, info = env.step(action)
```

In case you do not want the ground truth rewards and transition to be leaked to the agent, use the AnyMDPSolverOTS instead. This solver inplement a ideal environment modeling and a planning-based policy.

```python
from xenoverse.anymdp import AnyMDPSolverQ, AnyMDPSolverOTS

 # AnyMDPSolverOTS solves the MDP with Value Iteration and Optimal Thompson Sampling (Gaussian Noise)
solver = AnyMDPSolverOTS(env) 

# AnyMDPSolverQ solves the MDP with Q-learning and Optimal Thompson Sampling (Gaussian Noise)
#solver = AnyMDPSolverQ(env) 

state, info = env.reset()
done = False
while not done:
    action = solver.policy(state)
    state, reward, done, info = env.step(action)
    solver.learner(state, action, next_state, reward, done) # update the learner
```

## Procedurely generating an MDP task

The transition matrix and reward matrix are procedurally generated by [task_sampler](task_sampler.py). It is generated by randomly sample nodes in a high dimensional space. The transition matrix and the reward matrix are further checked to make sure it is a non-trival problem to be solved.
You can visualize each task by runing ``task_visualizer()`` from ``visualizer.py``, by which you will get the following images representing nodes and transitions. Notice that we can not visualize all the information, the transition and reward matrix are averaged over the actions and source states.

![AnyMDP Task Visualizer](https://github.com/FutureAGI/DataPack/blob/main/demo/anymdp/AnyMDP_Visualization.png) 